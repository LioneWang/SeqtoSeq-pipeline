# ğŸš€ Seq2Seq-pipeline: RNN vs. Transformer æ·±åº¦å¯¹æ¯”

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=yellow)
![PyTorch](https://img.shields.io/badge/PyTorch-1.10%2B-orange?logo=pytorch)
![License](https://img.shields.io/badge/License-MIT-green)
![Dataset](https://img.shields.io/badge/Dataset-WMT%20News%20v14-blue)

è¿™æ˜¯ä¸€ä¸ªç”¨äº**ç¥ç»æœºå™¨ç¿»è¯‘ (NMT)** çš„ç«¯åˆ°ç«¯ Seq2Seq æµç¨‹ã€‚æœ¬é¡¹ç›®çš„æ ¸å¿ƒæ˜¯**æ·±å…¥æ¯”è¾ƒ**ä¸¤ç§é‡Œç¨‹ç¢‘å¼çš„æ¶æ„ï¼š
1.  **ç»å…¸ RNN + Attention æ¨¡å‹**
2.  **ç°ä»£ Transformer æ¨¡å‹**

æœ¬é¡¹ç›®å®ç°äº†ä»åŸå§‹æ–‡æœ¬ä¸‹è½½ã€æ•°æ®é¢„å¤„ç†ã€BPE åˆ†è¯å™¨è®­ç»ƒã€æ¨¡å‹è®­ç»ƒã€æ¨ç†(è§£ç )åˆ°æœ€ç»ˆç»“æœå¯è§†åŒ–çš„å®Œæ•´å·¥ä½œæµã€‚

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

* **ç«¯åˆ°ç«¯æµç¨‹ (End-to-End):** ä»…éœ€åŸå§‹æ–‡æœ¬æ–‡ä»¶ï¼Œå³å¯ä¸€é”®å®Œæˆæ‰€æœ‰æ­¥éª¤ã€‚
* **æ¨¡å‹å¯¹æ¯” (Model Comparison):** åœ¨åŒä¸€æ•°æ®é›†å’Œé¢„å¤„ç†æµç¨‹ä¸‹ï¼Œå…¬å¹³å¯¹æ¯” RNN+Attention å’Œ Transformer çš„æ€§èƒ½ã€‚
* **ç°ä»£åˆ†è¯ (Modern Tokenization):** ä½¿ç”¨ `SentencePiece` (BPE) è®­ç»ƒæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„ç‹¬ç«‹åˆ†è¯å™¨ï¼Œé«˜æ•ˆå¤„ç† OOVï¼ˆæœªç™»å½•è¯ï¼‰é—®é¢˜ã€‚
* **æ¨¡å—åŒ–ä»£ç  (Modular Code):** æ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œå°†æ¨¡å‹ (`nmt_model.py`, `transformer.py`)ã€å·¥å…· (`utils.py`) å’Œæ‰§è¡Œè„šæœ¬ (`run.py`, `vocab.py`) åˆ†ç¦»ã€‚
* **å¯è§†åŒ– (Visualization):** åŒ…å« Jupyter Notebookï¼Œç”¨äºç»˜åˆ¶è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿ã€PPL æ›²çº¿ï¼Œä»¥åŠï¼ˆå¯é€‰çš„ï¼‰æ³¨æ„åŠ›çƒ­å›¾ã€‚

---

## ğŸ§  æ¶æ„å¯¹å†³ï¼šRNN vs. Transformer

æœ¬é¡¹ç›®åœ¨ **ä¸­è‹±(ZH-EN)** ç¿»è¯‘ä»»åŠ¡ä¸Šå¯¹ä»¥ä¸‹ä¸¤ç§æ¶æ„è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼š

### 1. RNN + Attention (ç»å…¸ç»„åˆ)
* **ç¼–ç å™¨ (Encoder):** åŒå‘ LSTM (æˆ– GRU)ï¼Œæ•æ‰åºåˆ—çš„æ—¶åºä¿¡æ¯ã€‚
* **è§£ç å™¨ (Decoder):** å•å‘ LSTM (æˆ– GRU)ï¼Œåœ¨æ¯ä¸€æ­¥ç”Ÿæˆå•è¯ã€‚
* **æ³¨æ„åŠ› (Attention):** é‡‡ç”¨ Luong æˆ– Bahdanau æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿è§£ç å™¨èƒ½å¤Ÿâ€œå…³æ³¨â€æºå¥å­çš„ä¸åŒéƒ¨åˆ†ã€‚
* **ç“¶é¢ˆ:** RNN çš„æ—¶åºä¾èµ–æ€§ä½¿å…¶éš¾ä»¥å¹¶è¡Œè®¡ç®—ï¼Œåœ¨é•¿åºåˆ—ä¸Šå®¹æ˜“ä¸¢å¤±ä¿¡æ¯ã€‚

### 2. Transformer (SOTA åŸºç¡€)
* **æ ¸å¿ƒ:** å®Œå…¨æŠ›å¼ƒ RNNï¼Œä»…ä¾èµ–**è‡ªæ³¨æ„åŠ› (Self-Attention)** å’Œ**å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)**ã€‚
* **ç¼–ç å™¨ (Encoder):** N å±‚ Encoder-Layer å †å ï¼Œå¹¶è¡Œå¤„ç†æ•´ä¸ªè¾“å…¥åºåˆ—ã€‚
* **è§£ç å™¨ (Decoder):** N å±‚ Decoder-Layer å †å ï¼Œä½¿ç”¨ Masked Self-Attention æ¥é˜²æ­¢â€œä½œå¼Šâ€ã€‚
* **ä¼˜åŠ¿:** å¼ºå¤§çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå“è¶Šçš„é•¿è·ç¦»ä¾èµ–æ•æ‰èƒ½åŠ›ï¼Œå·²æˆä¸º BERTã€GPT ç­‰ç°ä»£é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ã€‚

---

## ğŸ“š æ•°æ®é›†

æœ¬é¡¹ç›®ä½¿ç”¨ [WMT News Commentary v14](https://data.statmt.org/news-commentary/v14/) æ•°æ®é›†ã€‚

* **è¯­è¨€å¯¹:** `Chinese (ZH) - English (EN)`
* **é¢„å¤„ç†:** åŸå§‹æ•°æ®ç»è¿‡æ¸…ç†ã€è¿‡æ»¤ï¼ˆä¾‹å¦‚ï¼Œç§»é™¤è¿‡é•¿/è¿‡çŸ­å¥å­ï¼‰å’Œè§„èŒƒåŒ–ã€‚

---

## ğŸ“Š æ€§èƒ½ç»“æœ (ç¤ºä¾‹)

åœ¨**ä¸­è‹±ç¿»è¯‘**ä»»åŠ¡ä¸Šï¼Œä¸¤ç§æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”å¦‚ä¸‹ã€‚Transformer åœ¨ BLEU å’Œ PPL ä¸Šå‡è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶æ¶æ„çš„å…ˆè¿›æ€§ã€‚

| æ¨¡å‹ | éªŒè¯é›† PPL (Perplexity) â†“ | æµ‹è¯•é›† BLEU Score â†‘ |
| :--- | :---: | :---: |
| RNN + Attention | 18.24 | 24.5 |
| **Transformer (Base)** | **9.15** | **31.2** |

### ğŸ“ˆ è®­ç»ƒå¯è§†åŒ–

(è¯·åœ¨æ­¤å¤„æ›¿æ¢ä¸ºä½ è‡ªå·±çš„è®­ç»ƒæ›²çº¿å›¾)

``

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
Seq2Seq-pipeline/
â”‚
â”œâ”€â”€ data/                    # (è‡ªåŠ¨åˆ›å»º) å­˜æ”¾åŸå§‹å’Œå¤„ç†åçš„æ•°æ®
â”‚   â”œâ”€â”€ train.zh
â”‚   â””â”€â”€ train.en
â”‚
â”œâ”€â”€ outputs/                 # (è‡ªåŠ¨åˆ›å»º) å­˜æ”¾æ¨ç†ç»“æœå’Œæ—¥å¿—
â”‚   â”œâ”€â”€ ppl.log
â”‚   â””â”€â”€ test_outputs.txt
â”‚
â”œâ”€â”€ src.model                # (è‡ªåŠ¨ç”Ÿæˆ) æºè¯­è¨€ BPE åˆ†è¯æ¨¡å‹
â”œâ”€â”€ tgt.model                # (è‡ªåŠ¨ç”Ÿæˆ) ç›®æ ‡è¯­è¨€ BPE åˆ†è¯æ¨¡å‹
â”œâ”€â”€ vocab_zh_en.json         # (è‡ªåŠ¨ç”Ÿæˆ) ç»„åˆè¯æ±‡è¡¨
â”‚
â”œâ”€â”€ nmt_model.py             # RNN + Attention æ¨¡å‹çš„å®šä¹‰
â”œâ”€â”€ transformer.py           # Transformer æ¨¡å‹çš„å®šä¹‰
â”œâ”€â”€ vocab.py                 # è¯æ±‡è¡¨å’Œ BPE åˆ†è¯å™¨æ„å»ºè„šæœ¬
â”œâ”€â”€ utils.py                 # æ•°æ®è¯»å–ã€æ‰¹å¤„ç†ç­‰å·¥å…·å‡½æ•°
â”œâ”€â”€ run.py                   # è®­ç»ƒå’Œæ¨ç†çš„ä¸»æ‰§è¡Œè„šæœ¬
â”‚
â”œâ”€â”€ visualization.ipynb      # ç”¨äºç»˜åˆ¶å›¾è¡¨å’Œåˆ†æç»“æœçš„ Notebook
â”œâ”€â”€ requirements.txt         # Python ä¾èµ–
â””â”€â”€ README.md                # ä½ æ­£åœ¨çœ‹çš„è¿™ä¸ªæ–‡ä»¶
```

---

## ğŸ’» å®‰è£…ä¸ç¯å¢ƒ

1.  å…‹éš†æœ¬ä»“åº“ï¼š
    ```bash
    git clone [https://github.com/YOUR_USERNAME/SeqtoSeq-pipeline.git](https://github.com/YOUR_USERNAME/SeqtoSeq-pipeline.git)
    cd SeqtoSeq-pipeline
    ```

2.  (æ¨è) åˆ›å»ºå¹¶æ¿€æ´» Conda ç¯å¢ƒï¼š
    ```bash
    conda create -n nmt python=3.9
    conda activate nmt
    ```

3.  å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š
    ```bash
    # ç¡®ä¿ä½ å·²å®‰è£… PyTorch ([https://pytorch.org/](https://pytorch.org/))
    pip install -r requirements.txt
    ```
    ( `requirements.txt` åº”è‡³å°‘åŒ…å«: `torch`, `numpy`, `tqdm`, `sentencepiece`, `sacrebleu`, `jupyter` )

---

## ğŸš€ è¿è¡Œå®Œæ•´æµç¨‹

æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¤ç°æ•´ä¸ªå®éªŒï¼š

### æ­¥éª¤ 1: ä¸‹è½½å’Œå‡†å¤‡æ•°æ®

(è¯·åœ¨æ­¤å¤„æ·»åŠ ä½ çš„æ•°æ®ä¸‹è½½è¯´æ˜ï¼Œä¾‹å¦‚ï¼š)
```bash
# å‡è®¾ä½ æœ‰ä¸€ä¸ªè„šæœ¬æ¥ä¸‹è½½å’Œè§£å‹æ•°æ®
bash scripts/get_data.sh
# è¿™å°†ä¼šåœ¨ data/ ç›®å½•ä¸‹åˆ›å»º train.zh å’Œ train.en
```

### æ­¥éª¤ 2: è®­ç»ƒåˆ†è¯å™¨ & æ„å»ºè¯æ±‡è¡¨

æ­¤æ­¥éª¤å°†è®­ç»ƒ `src.model` å’Œ `tgt.model`ï¼Œå¹¶ç”Ÿæˆ `vocab_zh_en.json`ã€‚

```bash
python vocab.py \
    --train-src=data/train.zh \
    --train-tgt=data/train.en \
    --src-vocab-size=21000 \
    --tgt-vocab-size=8000 \
    vocab_zh_en.json
```

### æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹

ä½ å¯ä»¥é€‰æ‹©è®­ç»ƒ RNN æˆ– Transformerã€‚

#### é€‰é¡¹ A: è®­ç»ƒ RNN + Attention
```bash
python run.py train \
    --model-type=rnn \
    --train-src=data/train.zh \
    --train-tgt=data/train.en \
    --dev-src=data/dev.zh \
    --dev-tgt=data/dev.en \
    --vocab=vocab_zh_en.json \
    --cuda \
    --lr=5e-4 \
    --batch-size=32 \
    --save-to=model_rnn.bin
```

#### é€‰é¡¹ B: è®­ç»ƒ Transformer
```bash
python run.py train \
    --model-type=transformer \
    --train-src=data/train.zh \
    --train-tgt=data/train.en \
    --dev-src=data/dev.zh \
    --dev-tgt=data/dev.en \
    --vocab=vocab_zh_en.json \
    --cuda \
    --lr=3e-4 \
    --batch-size=64 \
    --save-to=model_transformer.bin
```

### æ­¥éª¤ 4: æ¨ç† & è¯„ä¼° (BLEU)

ä½¿ç”¨ä½ è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè§£ç ã€‚

```bash
python run.py decode \
    --model-type=transformer \
    --model-path=model_transformer.bin \
    --test-src=data/test.zh \
    --test-tgt=data/test.en \
    --output-file=outputs/test_outputs.txt \
    --cuda
```
è„šæœ¬å°†è‡ªåŠ¨åœ¨ `test.en` (å‚è€ƒ) å’Œ `test_outputs.txt` (å‡è®¾) ä¹‹é—´è®¡ç®—**è¯­æ–™åº“ BLEU åˆ†æ•°**ã€‚

### æ­¥éª¤ 5: å¯è§†åŒ–ç»“æœ

æ‰“å¼€ Jupyter Notebook æ¥åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„ `ppl.log` æˆ– `dev_ppl.log`ã€‚

```bash
jupyter notebook visualization.ipynb
```

---

## ğŸ’¡ æœªæ¥å·¥ä½œ

* [ ] å®ç° Beam Search (é›†æŸæœç´¢) ä»¥æå‡è§£ç è´¨é‡ã€‚
* [ ] é›†æˆ TensorBoard è¿›è¡Œå®æ—¶è®­ç»ƒç›‘æ§ã€‚
* [ ] æ‰©å±•ä»¥æ”¯æŒå…¶ä»–è¯­è¨€å¯¹ï¼ˆä¾‹å¦‚ DE-ENï¼‰ã€‚

---

## è‡´è°¢

æœ¬é¡¹ç›®çš„ä»£ç ç»“æ„å’Œéƒ¨åˆ†å®ç°æ·±å— Stanford CS224n (NLP with Deep Learning) è¯¾ç¨‹ä½œä¸šçš„å¯å‘ã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE)ã€‚
